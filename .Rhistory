# By tidy, we mean that every row should represent an observation, and every column should represent a variable measured during an observation
# We can view a dataframe called iris that comes with the tidyverse to see what this means
iris
# We see that every row represents a distinct flower, and that every columns gives info on a different aspect of the flower
# You may have noticed that the output of iris is quite verbose.
# Standard R dataframes are great, but they're 25 years old, and new and improved versions are available with the tidyverse.
# We will use an "updated" form of dataframe that comes wtih the tidyverse, known as a tibble
iris_tibble <- as_tibble(iris)
iris_tibble
# We see that the output of a tibble is more concise than that of a generic dataframe
# Wer'e not overwhelemd with lots of rows, and it's clear what each column represents
# Tibbles are also easier to modify, manipulate and create than generic dataframes.
# Data cleaning and manipulation will be the focus of the next lesson.
# today we will focus on something more fun: getting started with visualizing data with ggplot2
# The "gg" in ggplot stand for grammar of graphics.
# The basic idea is that you build your plot in layers, adding more and more detail with each layer.
#  This may seems a bit abstract for a simple plot, but as you'll see it's quite powerful.
# The most basic plots in ggplot require that you specify the data, and then the type of plot, or plot geometry.
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width))
#here we've loaded in our iris tibble and used geom_point to tell ggplot that we'd like a scatter plot
# The mapping inside geom_point tells us what variables we'd like to assign to the x and y coordinates in the scatter plot
# Note that ggplot works for any dataframe, not just a tibble.
#If we had used iris instead of iris_tibble we'd see the same thing.
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width))
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width, color = Species))
# note that, once we specify the geometry of the plots we'd like, we need to tell ggplot how we'd like to map the variables in our dataframe to different aesthetics of the plot.
# In this plot we've mapped the x and y coordinates of points in the scatter plot to sepal length and width, respectively.
# the third argument in aes, color, tells ggplot to color each species of flower differently, so we can easily distinguish them in the plot.
# Note that while sepal length and width are numeric variables, species is a categorical variable.
# The way we analyze categorical and numeric (continuous) variables is often fundamentally different.
# For instance, when exploring the relation between two continuous variables, plotting them as the x and y coordinates of a scatter plot as we've done here often makes sense.
# In order to model this relation we might fit a line (or some other curve) to the scatter plot
#sepal_model <- lm(Sepal.Width~Sepal.Length)
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width, color = Species)) +
geom_smooth(method = "lm",aes(x =Sepal.Length, y = Sepal.Width), se = FALSE, color = "black")
# Clearly this line isn't a great fit to the data.  We can do a bit better by fitting a line to each species
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width, color = Species)) +
geom_smooth(method = "lm",aes(x =Sepal.Length, y = Sepal.Width, color = Species), se = FALSE)
# Rather than comparing two continuous variable, we may want to look at the histogram of a single continuous variable
ggplot(data = iris_tibble) + geom_histogram(mapping = aes(x =Sepal.Length))
# Another option here would be to display a separate sepal length histogram for each species
ggplot(data = iris_tibble) + geom_histogram(mapping = aes(x =Sepal.Length, color = Species))
# Other options for histogram visualization by species include...
# Another good option is to summarize the histogram of sepal length for each species using boxplots
ggplot(data = iris_tibble) + geom_boxplot(mapping = aes(x =Sepal.Length, color = Species))
#NEXT SHOW VIOLIN PLOT
ggplot(data = iris_tibble) + geom_violin(mapping = aes(x =Sepal.Length, y = Species, color = Species))
# WE CAN ALSO LOOK AT RAW NUMBER OF OBSERVATION PER CATEGORY USING A BAR CHART
ggplot(data = iris_tibble) + geom_bar(mapping = aes(x =Species, color = Species))
# We see that each species had exactly 50 observations.  We can change the color scheme to fill to make things a bit prettier
ggplot(data = iris_tibble) + geom_bar(mapping = aes(x =Species, fill = Species))
# We can make plots like this more interesting once we learn how to filter tibbles in the next lesson.
#We can use other methods to visualize the relation between pairs of categorical variables
# Let's take a look at another dataframe that comes with the tidyverse and has multiple categorical variables, mpg
mpg
mpg_tibble <- as_tibble(mpg)
mpg_tibble
# We can display the relations between categorical variables in a contingency table that displays the counts for a given set of category values
# We can visualize this contingency table graphically using a heat map in ggplot
# For instance, for the categorical varibales manufacturer and class in mpg, the heat map look like
ggplot(data = mpg) + geom_tile(aes(y = manufacturer, x= class ))
# Just as we can fit a curve to a scatter plot relating two numerical variables, we can model the relation between categorical variables with logistic regression and other techniques
# We'll cover modelling relations between categorical variables when we build classification models in future videos
#******MENTION FACET WRAP, log scale, removing grey lines, varying point size and shape saving as pdf.
ggplot(data = mpg) + geom_tile(aes( x= class, y = manufacturer ))
ggplot(data = mpg) + geom_raster(aes( x= class, y = manufacturer ))
ggplot(data = mpg) + geom_raster(aes( x= class, y = manufacturer ))
ggplot(data = mpg) + geom_tile(aes( x= class, y = manufacturer ))
ggplot(data = mpg) + geom_rect(aes( x= class, y = manufacturer ))
ggplot(data = mpg) + geom_tile(aes( x= class, y = manufacturer ))
ggplot(data = mpg) + geom_tile(aes( x= class, y = manufacturer, fill = density ))
ggplot(data = mpg) + geom_tile(aes( x= class, y = manufacturer, fill = count() ))
ggplot(data = mpg) + geom_tile(aes( x= class, y = manufacturer, fill = count ))
ggplot(data = mpg) + geom_tile(aes( x= class, y = manufacturer, fill = n() ))
ggplot(data = mpg) + geom_bind2d(aes( x= class, y = manufacturer))
ggplot(data = mpg) + geom_bin2d(aes( x= class, y = manufacturer))
# Now that we've become familiar with the bare bones of R, we can start looking at more sophisticated tools the language provides for data science
# One of the tools we'll need is called a dataframe, which is essentially a 2D array with named columns that can be accesed by name.
# There is a bit more to the distinction between dataframes and 2D arrays (matrices), like in an dataframe each column can be a different data type.
# Intuitively though, it's fine to think of a dataframe as a special type of array.
#We can create an array of a give size full of zeros using the matrix command we've seen before.
num_rows <- 7
num_cols <- 5
example_matrix <- matrix(0, num_rows, num_cols)
# And then convert this matrix to a dataframe
example_dataframe <- data.frame(example_matrix)
# We can access single elements,
nrow <- 2
ncol <- 3
example_dataframe[2,3]
# We can also access an entire column or row, which itself is a one dimensional dataframe
example_dataframe[,ncol]
example_dataframe[nrow,]
# Notice that each of the columns has a name in a dataframe.  We can change the names of columns via
colnames(example_dataframe) <- c("column1","column2", "column3")
# We'll now see the updated column names when we view the dataframe again
example_dataframe
# And we can access the columns by name.  When we do this they'll appear as vectors
example_dataframe$column1
# Dataframes are great for working with data because you can manipulate entire rows based on the entry of a single column
# To get the most out of dataframes, we can download an install a set of R packages designed specifically for data science
# install.packages("tidyverse")
library(tidyverse)
# The major idea behind the tidyverse is that data should be in "tidy" format whenever possible
# By tidy, we mean that every row should represent an observation, and every column should represent a variable measured during an observation
# We can view a dataframe called iris that comes with the tidyverse to see what this means
iris
# We see that every row represents a distinct flower, and that every columns gives info on a different aspect of the flower
# You may have noticed that the output of iris is quite verbose.
# Standard R dataframes are great, but they're 25 years old, and new and improved versions are available with the tidyverse.
# We will use an "updated" form of dataframe that comes wtih the tidyverse, known as a tibble
iris_tibble <- as_tibble(iris)
iris_tibble
# We see that the output of a tibble is more concise than that of a generic dataframe
# We're not overwhelemd with lots of rows, and it's clear what each column represents
# Tibbles are also easier to modify, manipulate and create than generic dataframes.
# Data cleaning and manipulation will be the focus of the next lesson.
# today we will focus on something more fun: getting started with visualizing data with ggplot2
# The "gg" in ggplot stand for grammar of graphics.
# The basic idea is that you build your plot in layers, adding more and more detail with each layer.
# This may seems a bit abstract for a simple plot, but as you'll see it's quite powerful.
# The most basic plots in ggplot require that you specify the data, and then the type of plot, or plot geometry.
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width))
#here we've loaded in our iris tibble and used geom_point to tell ggplot that we'd like a scatter plot
# The mapping inside geom_point tells us what variables we'd like to assign to the x and y coordinates in the scatter plot
# Note that ggplot works for any dataframe, not just a tibble.
# If we had used iris instead of iris_tibble we'd see the same thing.
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width))
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width, color = Species))
# note that, once we specify the geometry of the plots we'd like, we need to tell ggplot how we'd like to map the variables in our dataframe to different aesthetics of the plot.
# In this plot we've mapped the x and y coordinates of points in the scatter plot to sepal length and width, respectively.
# the third argument in aes, color, tells ggplot to color each species of flower differently, so we can easily distinguish them in the plot.
# Note that while sepal length and width are numeric variables, species is a categorical variable.
# The way we analyze categorical and numeric (continuous) variables is sometimes different.
# For instance, when exploring the relation between two continuous variables, plotting them as the x and y coordinates of points on  a scatter plot makes sense.
# In order to model this relation we might fit a line (or some other curve) to the scatter plot
#sepal_model <- lm(Sepal.Width~Sepal.Length)
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width, color = Species)) +
geom_smooth(method = "lm",aes(x =Sepal.Length, y = Sepal.Width), se = FALSE, color = "black")
# Clearly this line isn't a great fit to the data.  We can do a bit better by fitting a line to each species
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width, color = Species)) +
geom_smooth(method = "lm",aes(x =Sepal.Length, y = Sepal.Width, color = Species), se = FALSE)
# Rather than comparing two continuous variable, we may want to look at the histogram of a single continuous variable
ggplot(data = iris_tibble) + geom_histogram(mapping = aes(x =Sepal.Length))
# Another option here would be to display a separate sepal length histogram for each species
ggplot(data = iris_tibble) + geom_histogram(mapping = aes(x =Sepal.Length, color = Species))
# Another good option is to summarize the histogram of sepal length for each species using boxplots
ggplot(data = iris_tibble) + geom_boxplot(mapping = aes(x =Sepal.Length, color = Species))
#NEXT SHOW VIOLIN PLOT
ggplot(data = iris_tibble) + geom_violin(mapping = aes(x =Sepal.Length, y = Species, color = Species))
# WE CAN ALSO LOOK AT RAW NUMBER OF OBSERVATION PER CATEGORY USING A BAR CHART
ggplot(data = iris_tibble) + geom_bar(mapping = aes(x =Species, color = Species))
# We see that each species had exactly 50 observations.  We can change the color scheme to fill to make things a bit prettier
ggplot(data = iris_tibble) + geom_bar(mapping = aes(x =Species, fill = Species))
# We can make plots like this more interesting once we learn how to filter tibbles in the next lesson.
#We can use other methods to visualize the relation between pairs of categorical variables
# Let's take a look at another dataframe that comes with the tidyverse and has multiple categorical variables, mpg
mpg
mpg_tibble <- as_tibble(mpg)
mpg_tibble
# We can display the relations between categorical variables in a contingency table that displays the counts for a given set of category values
# We can visualize this contingency table graphically using a heat map in ggplot
# For instance, for the categorical varibales manufacturer and class in mpg, the heat map look like
ggplot(data = mpg) + geom_bin2d(aes( x= class, y = manufacturer))
# Just as we can fit a curve to a scatter plot relating two numerical variables, we can model the relation between categorical variables with logistic regression and other techniques
# We'll cover modelling relations between categorical variables when we build classification models in future videos
#******MENTION FACET WRAP, log scale, removing grey lines, varying point size and shape saving as pdf.
# Now that we've become familiar with the bare bones of R, we can start looking at more sophisticated tools the language provides for data science
# One of the tools we'll need is called a dataframe, which is essentially a 2D array with named columns that can be accesed by name.
# There is a bit more to the distinction between dataframes and 2D arrays (matrices), like in an dataframe each column can be a different data type.
# Intuitively though, it's fine to think of a dataframe as a special type of array.
#We can create an array of a give size full of zeros using the matrix command we've seen before.
num_rows <- 7
num_cols <- 5
example_matrix <- matrix(0, num_rows, num_cols)
# And then convert this matrix to a dataframe
example_dataframe <- data.frame(example_matrix)
# We can access single elements,
nrow <- 2
ncol <- 3
example_dataframe[2,3]
# We can also access an entire column or row, which itself is a one dimensional dataframe
example_dataframe[,ncol]
example_dataframe[nrow,]
# Notice that each of the columns has a name in a dataframe.  We can change the names of columns via
colnames(example_dataframe) <- c("column1","column2", "column3")
# We'll now see the updated column names when we view the dataframe again
example_dataframe
# And we can access the columns by name.  When we do this they'll appear as vectors
example_dataframe$column1
# Dataframes are great for working with data because you can manipulate entire rows based on the entry of a single column
# To get the most out of dataframes, we can download an install a set of R packages designed specifically for data science
# install.packages("tidyverse")
library(tidyverse)
# The major idea behind the tidyverse is that data should be in "tidy" format whenever possible
# By tidy, we mean that every row should represent an observation, and every column should represent a variable measured during an observation
# We can view a dataframe called iris that comes with the tidyverse to see what this means
iris
# We see that every row represents a distinct flower, and that every columns gives info on a different aspect of the flower
# You may have noticed that the output of iris is quite verbose.
# Standard R dataframes are great, but they're 25 years old, and new and improved versions are available with the tidyverse.
# We will use an "updated" form of dataframe that comes wtih the tidyverse, known as a tibble
iris_tibble <- as_tibble(iris)
iris_tibble
# We see that the output of a tibble is more concise than that of a generic dataframe
# We're not overwhelemd with lots of rows, and it's clear what each column represents
# Tibbles are also easier to modify, manipulate and create than generic dataframes.
# Data cleaning and manipulation will be the focus of the next lesson.
# today we will focus on something more fun: getting started with visualizing data with ggplot2
# The "gg" in ggplot stand for grammar of graphics.
# The basic idea is that you build your plot in layers, adding more and more detail with each layer.
# This may seems a bit abstract for a simple plot, but as you'll see it's quite powerful.
# The most basic plots in ggplot require that you specify the data, and then the type of plot, or plot geometry.
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width))
#here we've loaded in our iris tibble and used geom_point to tell ggplot that we'd like a scatter plot
# The mapping inside geom_point tells us what variables we'd like to assign to the x and y coordinates in the scatter plot
# Note that ggplot works for any dataframe, not just a tibble.
# If we had used iris instead of iris_tibble we'd see the same thing.
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width))
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width, color = Species))
# note that, once we specify the geometry of the plots we'd like, we need to tell ggplot how we'd like to map the variables in our dataframe to different aesthetics of the plot.
# In this plot we've mapped the x and y coordinates of points in the scatter plot to sepal length and width, respectively.
# the third argument in aes, color, tells ggplot to color each species of flower differently, so we can easily distinguish them in the plot.
# Note that while sepal length and width are numeric variables, species is a categorical variable.
# In order to model this relation we might fit a line (or some other curve) to the scatter plot
#sepal_model <- lm(Sepal.Width~Sepal.Length)
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width, color = Species)) +
geom_smooth(method = "lm",aes(x =Sepal.Length, y = Sepal.Width), se = FALSE, color = "black")
# Clearly this line isn't a great fit to the data.  We can do a bit better by fitting a line to each species
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width, color = Species)) +
geom_smooth(method = "lm",aes(x =Sepal.Length, y = Sepal.Width, color = Species), se = FALSE)
# We can also change the scale of the axes from linear to (natural) log
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width, color = Species)) +
geom_smooth(method = "lm",aes(x =Sepal.Length, y = Sepal.Width, color = Species), se = FALSE) + scale_y_log10() + scale_x_log10()
# Rather than comparing two continuous variable, we may want to look at the histogram of a single continuous variable
ggplot(data = iris_tibble) + geom_histogram(mapping = aes(x =Sepal.Length))
# Another option here would be to display a separate sepal length histogram for each species
ggplot(data = iris_tibble) + geom_histogram(mapping = aes(x =Sepal.Length, color = Species))
# Another good option is to summarize the histogram of sepal length for each species using boxplots
ggplot(data = iris_tibble) + geom_boxplot(mapping = aes(x =Sepal.Length, color = Species))
#  If we want more detail than the boxplot, provides, we can view a violoin plot, which shows the shape of the density for each species
ggplot(data = iris_tibble) + geom_violin(mapping = aes(x =Sepal.Length, y = Species, color = Species))
# WE CAN ALSO LOOK AT RAW NUMBER OF OBSERVATION PER CATEGORY USING A BAR CHART
ggplot(data = iris_tibble) + geom_bar(mapping = aes(x =Species, color = Species))
# We see that each species had exactly 50 observations.  We can change the color scheme to fill to make things a bit prettier
ggplot(data = iris_tibble) + geom_bar(mapping = aes(x =Species, fill = Species))
# We can make plots like this more interesting once we learn how to filter tibbles in the next lesson.
# We can use other methods to visualize the relation between pairs of categorical variables
# Let's take a look at another dataframe that comes with the tidyverse and has multiple categorical variables, mpg
mpg
mpg_tibble <- as_tibble(mpg)
mpg_tibble
# We can display the relations between categorical variables in a contingency table that displays the counts for a given set of category values
# We can visualize this contingency table graphically using a heat map in ggplot
# For instance, for the categorical varibales manufacturer and class in mpg, the heat map look like
ggplot(data = mpg) + geom_bin2d(aes( x= class, y = manufacturer))
# Just as we can fit a curve to a scatter plot relating two numerical variables, we can model the relation between categorical variables with logistic regression and other techniques
# We'll cover modelling relations between categorical variables when we build classification models in future videos
#******MENTION FACET WRAP, log scale, removing grey lines, varying point size and shape saving as pdf.
ggplot(data = iris_tibble) + geom_point(mapping = aes(x =Sepal.Length, y = Sepal.Width, color = Species)) +
geom_smooth(method = "lm",aes(x =Sepal.Length, y = Sepal.Width, color = Species), se = FALSE) + scale_y_log10() + scale_x_log10()
ggplot(data = iris_tibble) + geom_histogram(mapping = aes(x =Sepal.Length, color = Species))
ggplot(data = iris_tibble) + geom_histogram(mapping = aes(x =Sepal.Length, color = Species)) + facet_wrap(~Species)
setwd("~/BB_bottleneck")
library(tidyverse)
library(argparse)
# Handle command line arguments first
parser <- ArgumentParser()
parser$add_argument("--file", type="character", default = "./example_data/donor_freqs_recip_freqs_and_reads.txt",
help="file containing variant frequencies and reads")
parser$add_argument("--plot_bool", type="logical", default= FALSE,
help="determines whether pdf plot exact_plot.pdf is produced or not")
parser$add_argument("--var_calling_threshold", type="double", default= 0.03,
help="variant calling threshold")
parser$add_argument("--Nb_min", type="integer", default= 1,
help="Minimum bottleneck value considered")
parser$add_argument("--Nb_max", type="integer", default= 200,
help="Maximum bottleneck value considered")
parser$add_argument("--confidence_level", type="double", default= .95,
help="Confidence level (determines bounds of confidence interval)")
args <- parser$parse_args()
if (args$file == "no_input_return_error" ) { stop("file with lists of donor and recipient frequencies is a required argument.", call.=FALSE)}
plot_bool  <- args$plot_bool # determines whether or not a plot of log likelihood vs bottleneck size is produced
var_calling_threshold  <- args$var_calling_threshold # variant calling threshold for frequency in recipient
Nb_min <-  args$Nb_min     # Minimum bottleneck size we consider
if(Nb_min < 1){Nb_min = 1} # preventing erros with Nb_min at 0 or lower
Nb_max <- args$Nb_max      # Maximum bottlebeck size we consider
confidence_level <- args$confidence_level  # determines width of confidence interval
donor_freqs_recip_freqs_and_reads_observed <- read.table(args$file)  #table of SNP frequencies and reads in donor and recipient
original_row_count <- nrow(donor_freqs_recip_freqs_and_reads_observed) # number of rows in raw table
donor_freqs_recip_freqs_and_reads_observed <- subset(donor_freqs_recip_freqs_and_reads_observed, donor_freqs_recip_freqs_and_reads_observed[, 1] >= var_calling_threshold)
new_row_count <- nrow(donor_freqs_recip_freqs_and_reads_observed) # number of rows in filtered table
if(new_row_count != original_row_count )
{print("WARNING:  Rows of the input file with donor frequency less than variant calling threshold have been removed during analysis. ")}
donor_freqs_observed <-as.data.frame(donor_freqs_recip_freqs_and_reads_observed[,1])
n_variants <- nrow(donor_freqs_recip_freqs_and_reads_observed)
recipient_total_reads <- as.data.frame(donor_freqs_recip_freqs_and_reads_observed[,3]) #read.table(args[2])
recipient_var_reads_observed <- as.data.frame(donor_freqs_recip_freqs_and_reads_observed[,4])#read.table(args[3])
freqs_tibble <- tibble(donor_freqs = donor_freqs_recip_freqs_and_reads_observed[,1],
recip_total_reads = donor_freqs_recip_freqs_and_reads_observed[,3],
recip_var_reads = donor_freqs_recip_freqs_and_reads_observed[,4] )
Log_Beta_Binom <- function(nu_donor, recip_total_reads, recip_var_reads, NB_SIZE)  # This function gives Log Likelihood for every SNP
{ LL_val_above <- 0 # used for recipient reads above calling threshold
LL_val_below <- 0 # used for recipient reads below calling threshold
for(k in 0:NB_SIZE){
alpha <- k + 10^-9
beta <- (NB_SIZE - k) + 10^-9
m <- alpha/(alpha + beta)
s <- (alpha + beta)
LL_val_above <-  LL_val_above + dbetabinom( recip_var_reads, recip_total_reads, m, s)*dbinom(k, size=NB_SIZE, prob= nu_donor)
LL_val_below <- LL_val_below + pbetabinom( floor(var_calling_threshold*recip_total_reads), recip_total_reads, m, s)*dbinom(k, size=NB_SIZE, prob= nu_donor)
}
LL_val <- if_else(recip_var_reads >=  var_calling_threshold*recip_total_reads , LL_val_above,  LL_val_below )
# We use LL_val_above above the calling threshold, and LL_val_below below the calling threshold
LL_val <- log(LL_val) # convert likelihood to log likelihood
return(LL_val)
}
LL_func_approx <- function(Nb_size){  # This function sums over all SNP frequencies in the donor and recipient
Total_LL <- 0
LL_array <- Log_Beta_Binom(freqs_tibble$donor_freqs, freqs_tibble$recip_total_reads, freqs_tibble$recip_var_reads, Nb_size)
Total_LL <- sum(LL_array)
return(Total_LL)
}
# Now we define array of Log Likelihoods for all possible bottleneck sizes
LL_tibble <- tibble(bottleneck_size = c(Nb_min:Nb_max), Log_Likelihood = 0*c(Nb_min:Nb_max))
for(I in 1:nrow(LL_tibble) )
{LL_tibble$Log_Likelihood[I] <- LL_func_approx( LL_tibble$bottleneck_size[I] ) }
# Now we find the maximum likelihood estimate and the associated confidence interval
Max_LL <- max(LL_tibble$Log_Likelihood) # Maximum value of log likelihood
Max_LL_bottleneck <- which(LL_tibble$Log_Likelihood == max(LL_tibble$Log_Likelihood) ) # bottleneck size at which max likelihood occurs
likelihood_ratio <- qchisq(confidence_level, df=1) # necessary ratio of likelihoods set by confidence level
ci_tibble <- filter(LL_tibble, 2*(Max_LL - Log_Likelihood) <= likelihood_ratio )
lower_CI_bottleneck <- min(ci_tibble$bottleneck_size) # lower bound of confidence interval
upper_CI_bottleneck <- max(ci_tibble$bottleneck_size) # upper bound of confidence interval
# now we plot our results
if(plot_bool == FALSE){
ggplot(data = LL_tibble) + geom_point(aes(x = bottleneck_size, y= Log_Likelihood )) +
geom_vline(xintercept= Max_LL_bottleneck )  +
geom_vline(xintercept= lower_CI_bottleneck, color = "green" ) +
geom_vline(xintercept= upper_CI_bottleneck, color = "green" ) +
labs(x= "Bottleneck Size", y = "Log Likelihood" )
ggsave("exact_plot.jpg")
}
print("Bottleneck size")
print(Max_LL_bottleneck)
print("confidence interval left bound")
print(lower_CI_bottleneck)
print("confidence interval right bound")
print(upper_CI_bottleneck)
library(tidyverse)
library(argparse)
library(rmutil)
# Handle command line arguments first
parser <- ArgumentParser()
parser$add_argument("--file", type="character", default = "./example_data/donor_freqs_recip_freqs_and_reads.txt",
help="file containing variant frequencies and reads")
parser$add_argument("--plot_bool", type="logical", default= FALSE,
help="determines whether pdf plot exact_plot.pdf is produced or not")
parser$add_argument("--var_calling_threshold", type="double", default= 0.03,
help="variant calling threshold")
parser$add_argument("--Nb_min", type="integer", default= 1,
help="Minimum bottleneck value considered")
parser$add_argument("--Nb_max", type="integer", default= 200,
help="Maximum bottleneck value considered")
parser$add_argument("--confidence_level", type="double", default= .95,
help="Confidence level (determines bounds of confidence interval)")
args <- parser$parse_args()
if (args$file == "no_input_return_error" ) { stop("file with lists of donor and recipient frequencies is a required argument.", call.=FALSE)}
plot_bool  <- args$plot_bool # determines whether or not a plot of log likelihood vs bottleneck size is produced
var_calling_threshold  <- args$var_calling_threshold # variant calling threshold for frequency in recipient
Nb_min <-  args$Nb_min     # Minimum bottleneck size we consider
if(Nb_min < 1){Nb_min = 1} # preventing erros with Nb_min at 0 or lower
Nb_max <- args$Nb_max      # Maximum bottlebeck size we consider
confidence_level <- args$confidence_level  # determines width of confidence interval
donor_freqs_recip_freqs_and_reads_observed <- read.table(args$file)  #table of SNP frequencies and reads in donor and recipient
original_row_count <- nrow(donor_freqs_recip_freqs_and_reads_observed) # number of rows in raw table
donor_freqs_recip_freqs_and_reads_observed <- subset(donor_freqs_recip_freqs_and_reads_observed, donor_freqs_recip_freqs_and_reads_observed[, 1] >= var_calling_threshold)
new_row_count <- nrow(donor_freqs_recip_freqs_and_reads_observed) # number of rows in filtered table
if(new_row_count != original_row_count )
{print("WARNING:  Rows of the input file with donor frequency less than variant calling threshold have been removed during analysis. ")}
donor_freqs_observed <-as.data.frame(donor_freqs_recip_freqs_and_reads_observed[,1])
n_variants <- nrow(donor_freqs_recip_freqs_and_reads_observed)
recipient_total_reads <- as.data.frame(donor_freqs_recip_freqs_and_reads_observed[,3]) #read.table(args[2])
recipient_var_reads_observed <- as.data.frame(donor_freqs_recip_freqs_and_reads_observed[,4])#read.table(args[3])
freqs_tibble <- tibble(donor_freqs = donor_freqs_recip_freqs_and_reads_observed[,1],
recip_total_reads = donor_freqs_recip_freqs_and_reads_observed[,3],
recip_var_reads = donor_freqs_recip_freqs_and_reads_observed[,4] )
Log_Beta_Binom <- function(nu_donor, recip_total_reads, recip_var_reads, NB_SIZE)  # This function gives Log Likelihood for every SNP
{ LL_val_above <- 0 # used for recipient reads above calling threshold
LL_val_below <- 0 # used for recipient reads below calling threshold
for(k in 0:NB_SIZE){
alpha <- k + 10^-9
beta <- (NB_SIZE - k) + 10^-9
m <- alpha/(alpha + beta)
s <- (alpha + beta)
LL_val_above <-  LL_val_above + dbetabinom( recip_var_reads, recip_total_reads, m, s)*dbinom(k, size=NB_SIZE, prob= nu_donor)
LL_val_below <- LL_val_below + pbetabinom( floor(var_calling_threshold*recip_total_reads), recip_total_reads, m, s)*dbinom(k, size=NB_SIZE, prob= nu_donor)
}
LL_val <- if_else(recip_var_reads >=  var_calling_threshold*recip_total_reads , LL_val_above,  LL_val_below )
# We use LL_val_above above the calling threshold, and LL_val_below below the calling threshold
LL_val <- log(LL_val) # convert likelihood to log likelihood
return(LL_val)
}
LL_func_approx <- function(Nb_size){  # This function sums over all SNP frequencies in the donor and recipient
Total_LL <- 0
LL_array <- Log_Beta_Binom(freqs_tibble$donor_freqs, freqs_tibble$recip_total_reads, freqs_tibble$recip_var_reads, Nb_size)
Total_LL <- sum(LL_array)
return(Total_LL)
}
# Now we define array of Log Likelihoods for all possible bottleneck sizes
LL_tibble <- tibble(bottleneck_size = c(Nb_min:Nb_max), Log_Likelihood = 0*c(Nb_min:Nb_max))
for(I in 1:nrow(LL_tibble) )
{LL_tibble$Log_Likelihood[I] <- LL_func_approx( LL_tibble$bottleneck_size[I] ) }
# Now we find the maximum likelihood estimate and the associated confidence interval
Max_LL <- max(LL_tibble$Log_Likelihood) # Maximum value of log likelihood
Max_LL_bottleneck <- which(LL_tibble$Log_Likelihood == max(LL_tibble$Log_Likelihood) ) # bottleneck size at which max likelihood occurs
likelihood_ratio <- qchisq(confidence_level, df=1) # necessary ratio of likelihoods set by confidence level
ci_tibble <- filter(LL_tibble, 2*(Max_LL - Log_Likelihood) <= likelihood_ratio )
lower_CI_bottleneck <- min(ci_tibble$bottleneck_size) # lower bound of confidence interval
upper_CI_bottleneck <- max(ci_tibble$bottleneck_size) # upper bound of confidence interval
# now we plot our results
if(plot_bool == FALSE){
ggplot(data = LL_tibble) + geom_point(aes(x = bottleneck_size, y= Log_Likelihood )) +
geom_vline(xintercept= Max_LL_bottleneck )  +
geom_vline(xintercept= lower_CI_bottleneck, color = "green" ) +
geom_vline(xintercept= upper_CI_bottleneck, color = "green" ) +
labs(x= "Bottleneck Size", y = "Log Likelihood" )
ggsave("exact_plot.jpg")
}
print("Bottleneck size")
print(Max_LL_bottleneck)
print("confidence interval left bound")
print(lower_CI_bottleneck)
print("confidence interval right bound")
print(upper_CI_bottleneck)
library(tidyverse)
library(argparse)
# Handle command line arguments first
parser <- ArgumentParser()
parser$add_argument("--file", type="character", default = "./example_data/donor_and_recipient_freqs.txt" ,
help="file containing variant frequencies")
parser$add_argument("--plot_bool", type="logical", default= FALSE,
help="determines whether pdf plot approx_plot.pdf is produced or not")
parser$add_argument("--var_calling_threshold", type="double", default= 0.03,
help="variant calling threshold")
parser$add_argument("--Nb_min", type="integer", default= 1,
help="Minimum bottleneck value considered")
parser$add_argument("--Nb_max", type="integer", default= 200,
help="Maximum bottleneck value considered")
parser$add_argument("--confidence_level", type="double", default= .95,
help="Confidence level (determines bounds of confidence interval)")
args <- parser$parse_args()
# Now create necessary variables and dataframes
plot_bool  <- args$plot_bool # determines whether or not a plot of log likelihood vs bottleneck size is produced
var_calling_threshold  <- args$var_calling_threshold # variant calling threshold for frequency in recipient
Nb_min <- args$Nb_min      # Minimum bottleneck size we consider
if(Nb_min < 1){Nb_min = 1} # preventing erros with Nb_min at 0 or lower
Nb_max <-  args$Nb_max     # Maximum bottlebeck size we consider
confidence_level <- args$confidence_level # determines width of confidence interval
donor_and_recip_freqs_observed <- read.table(args$file) # table of SNP frequencies in donor and recipient
original_row_count <- nrow(donor_and_recip_freqs_observed)  # number of rows in raw table
donor_and_recip_freqs_observed <- subset(donor_and_recip_freqs_observed, donor_and_recip_freqs_observed[, 1] >= var_calling_threshold)
new_row_count <- nrow(donor_and_recip_freqs_observed) # number of rows in filtered table
if(new_row_count != original_row_count )
{print("WARNING:  Rows of the input file with donor frequency less than variant calling threshold have been removed during analysis. ")}
n_variants <- nrow(donor_and_recip_freqs_observed) # number of variants
freqs_tibble <- tibble(donor_freqs = donor_and_recip_freqs_observed[, 1], recip_freqs = donor_and_recip_freqs_observed[, 2] )
# Now implement the beta binomial algorithm
Log_Beta_Binom <- function(nu_donor, nu_recipient, NB_SIZE)  # This function gives Log Likelihood for every donor recipient SNP frequency pair
{ LL_val_above <- 0 # used for recipient frequencies above calling threshold
LL_val_below <- 0 # used for recipient frequencies below calling threshold
for(k in 0:NB_SIZE){
LL_val_above <-  LL_val_above +  dbeta(nu_recipient, k, (NB_SIZE - k) )*dbinom(k, size=NB_SIZE, prob= nu_donor)
LL_val_below <- LL_val_below +   pbeta(var_calling_threshold, k, (NB_SIZE - k))*dbinom(k,size=NB_SIZE, prob= nu_donor)
}
LL_val <- if_else(nu_recipient >= var_calling_threshold , LL_val_above,  LL_val_below )
# We use LL_val_above above the calling threshold, and LL_val_below below the calling threshold
LL_val <- log(LL_val) # convert likelihood to log likelihood
return(LL_val)
}
LL_func_approx <- function(Nb_size){  # This function sums over all SNP frequencies in the donor and recipient
Total_LL <- 0
LL_array <- Log_Beta_Binom(freqs_tibble$donor_freqs, freqs_tibble$recip_freqs, Nb_size)
Total_LL <- sum(LL_array)
return(Total_LL)
}
# Now we define array of Log Likelihoods for all possible bottleneck sizes
LL_tibble <- tibble(bottleneck_size = c(Nb_min:Nb_max), Log_Likelihood = 0*c(Nb_min:Nb_max))
for(I in 1:nrow(LL_tibble) )
{LL_tibble$Log_Likelihood[I] <- LL_func_approx( LL_tibble$bottleneck_size[I] ) }
# Now we find the maximum likelihood estimate and the associated confidence interval
Max_LL <- max(LL_tibble$Log_Likelihood) # Maximum value of log likelihood
Max_LL_bottleneck <- which(LL_tibble$Log_Likelihood == max(LL_tibble$Log_Likelihood) ) # bottleneck size at which max likelihood occurs
likelihood_ratio <- qchisq(confidence_level, df=1) # necessary ratio of likelihoods set by confidence level
ci_tibble <- filter(LL_tibble, 2*(Max_LL - Log_Likelihood) <= likelihood_ratio )
lower_CI_bottleneck <- min(ci_tibble$bottleneck_size) # lower bound of confidence interval
upper_CI_bottleneck <- max(ci_tibble$bottleneck_size) # upper bound of confidence interval
# now we plot our results
if(plot_bool == FALSE){
ggplot(data = LL_tibble) + geom_point(aes(x = bottleneck_size, y= Log_Likelihood )) +
geom_vline(xintercept= Max_LL_bottleneck )  +
geom_vline(xintercept= lower_CI_bottleneck, color = "green" ) +
geom_vline(xintercept= upper_CI_bottleneck, color = "green"  ) +
labs(x= "Bottleneck Size", y = "Log Likelihood" )
ggsave("approx_plot.jpg")
}
print("Bottleneck size")
print(Max_LL_bottleneck)
print("confidence interval left bound")
print(lower_CI_bottleneck)
print("confidence interval right bound")
print(upper_CI_bottleneck)
